---
layout: single
title: "Floating point 연산"
author_profile: true
excerpt: "floating point, fixed point"
categories:
    - GPU
---

CPU에는 FPU(floating point uint)가 있다. FPU는 부동소수점 연산을 효율적으로 처리하기 위한 하드웨어 논리회로 모듈이다. 부동소수점 실수에 대한 사칙 연산 및 거듭제곱 뿐만 아니라 삼각함수 등 다양한 수학 계산을 지원한다. 초기의 마이크로프로세서는 정수형 계산을 기반으로 하는 ALU만을 사용했었다. 그 후에 실수형 데이터 처리를 위해 전용 하드웨어인 논리 회로가 만들어졌다. 

초기 컴퓨터는 고정 소수점 방식으로 처리했다. 예를들어 아래 그림을 보면 정수부와 소수부를 나누어 표현을 하는데, 우리가 정말 정밀한 수를 표현하려고 하면 정수부가 아닌 소수부에서 더 많은 수 표현을 필요로 하는데 16bit로 표현을 하는 경우 꽤 제한이 될 수 있다. (그림은 32비트 운영체제 기준이라고 생각하면 된다.)

![fixed_point_32bit](/assets/images/fixed_point_32bit.png)

그래서 부동소수점이라는 방식이 등장하게 되는데, 이제 정수부와 소수부가 아닌, 지수부와 가수부로 나누어 표현을 하게 된다.

![floating_point_32bit](/assets/images/floating_point_32bit.png)

1.234 X 10^2 이라고 하면 가수부는 1.234가 되고 지수는 2가 된다. 여기서 꼭 알아두어야 할 것은 정확한 변환 방식이 아니라, 부동소수점 연산이 나오게 된 배경을 이해하자.

대부분 딥러닝에서 이루어지는 연산이 부동소수점으로 이루어지는데, 이러한 하드웨어의 구조와 관련이 있다고 볼 수 있을 것 같다.

의료영상을 다루다 보면 16bit 영상을 그대로 처리할 것인가 아니면 8bit 영상으로 변환하여 사용할 것인가 하는 문제가 있다. 일단 먼저 floating point에 의해서  16bit 영상도 디테일을 살려 충분히 처리 가능하다는 점이 있다. 하지만 보통의 CNN에서는 256x256 조금 크면 512x512로 resize 되어지기 때문에 Chest X ray의 경우 3007x3007에 비해서 최소 1/4 이상 줄어들 기 때문에 그만큼 디테일을 살리긴 힘들것이라고 본다. 다음에 16bit 영상과 8bit 영상의 학습 및 테스트 결과를 포스팅 해봐야겠다.