---
layout: single
title: "딥러닝의 올바른 학습의 방향 - Regularization"
author_profile: true
excerpt: "L1, L2"
categories:
    - CNN
---

딥러닝의 학습 목적이라고도 할 수 있는 일반화(Regularization)에 대해서 알아보자.

먼저 머신러닝에 쓰이는 전략 중에는 Test Error의 감소를 위해 설계된 것들이 많다. 심지어 훈련오차가 증가하는 대가를 치르더라도 Test Error를 줄이려는 전략들이 많이 있다. 이런 전략들들은 모두 Regularization 이라고 한다. 다시 한번 더 정리하면 Training Error를 줄이는 것이 아니라 Test Error를 줄이기 위해 학습 알고리즘에 가하는 모는 종류의 수정을 Regularization 이라고 할 수 있다.

딥러닝에서 대부분의 Regularization 전략은 일반화 추정량에 기초한다. 한 추정량의 일반화는 **bias를 늘리는 대신 variance을 줄이는 방식**으로 이루어진다. 즉 목표는 bias를 너무 증가시키지 않으면서 variance을 크게 줄여야 한다. 

그러면 어떠한 일반화 전략들이 있는지 살펴보도록 하자.


## 1. Parameter Norms Penalties

### L1 & L2 Loss fuction

![L2_loss](/assets/images/L2_loss.png)

![L1_loss](/assets/images/L1_loss.png)

L2 loss fuction을 보면 실제값과 예측값 오차의 제곱을 해주는 형태이다.  에러에 대한 패널티를 훨씬더 크게 설정한다고 볼 수 있다. 만약 outlier가 들어온 경우를 보면 L2의 경우에는 훨씬 더 패널티를 많이 받게 된다고 볼 수 있다. 모델의 Robustness의 관점에서 보면 L1 보다 L2가 더 떨어지는 셈이다.(Robustness는 이상치나 노이즈가 들어와도 크게 흔들리지 않는 것을 의미) outlier에 대한 영향을 적당히 무시하고 싶다면 L1 loss 를 이용하고, outlier를 충분히 고려해야 하는 상황이라면 L2 loss를 활용 할 수 있을 것이다.

### L1 Regularization & L2 Regularization

![L1_regular](/assets/images/L1_regular.png)

![L2_regular](/assets/images/L2_regular.png)

학습 시 사용하는 Loss fuction에 L1 or L2 Regularization가 더해진 식이다. 람다는 L1/L2 의 가중치다. 커질 수록 Regularization 비중을 높인다는 의미이다. 위에서도 언급했지만, 분산의 관점에서 더 설명하면, L2 Regularization의 경우 **“입력된 데이터의 분산이 실제보다 더 크다”** 라고 느끼게 만든다. 그래서 그 분산보다 낮은 분산을 갖는 feature들의 가중치를 감쇄시킨다. 

### Reference

- [http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)
- [https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions](https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions)
- [https://seongkyun.github.io/study/2019/04/18/l1_l2/](https://seongkyun.github.io/study/2019/04/18/l1_l2/)



