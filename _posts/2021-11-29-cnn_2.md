---
layout: single
title: "합성곱과 풀링의 의미"
author_profile: true
excerpt: "prior probability distribution"
categories:
    - CNN
---

### **사전확률분포 (Prior probability distribution)**

줄여서 사전분포는 모형의 매개변수들에 관한 하나의 확률분포인데, 바람직한 모형에 대한 우리의 사전 믿음을 의미한다. 즉, 그 어떤 자료도 관측하기 전에 우리가 가지고 있던, 주어진 과제에 바람직한 모형이 어떤 것인가에 관한 우리의 믿음을 확률분포 형태로 나타낸 것이다.

사전분포의 확률밀도가 얼마나 조밀한지에 따라 사전분포를 강하다 또는 약하다 라고 표현한다. 약한 사전분포는 엔트로피가 높은 사전분포이다. 이를테면 분산이 큰 가우스 분포가 약한 사전분포에 해당한다.  그런 사전분포를 적용하면 훈련 자료가 매개변수들을 다소 자유롭게 이동하게 된다. 강한 사전분포는 엔트로피가 아주 낮은 사전분포이다. 이를테면 분산이 작은 가우스 분포가 강한 사전분포에 해당한다. 그런 사전분포는 매개변수들의 최종값을 결정하는 데 좀 더 능동적인 역할을 한다. 무한히 강한 사전분포는 일부 매개변수 값들에 0의 확률을 부여함으로써 그런 매개 변수 값들이 절대로 허용되지 않음을 표현한다.

### 합성곱과 풀링 의미

합성곱 신경망은 완전 연결 신경망과 어떤점이 차이가 있는 것일까? 바로 무한히 강한 사전분포가 있는 신경망이라고 생각할 수 있다. (그 무한히 강한 사전분포는 필터를 의미한다) 즉, **합성곱을 사용한다는 것은 한 층의 매개변수들에 관한 하나의 무한히 강한 사전분포를 도입하는 것**이라고 할 수 있다.  그 층이 배워야 하는 함수에 오직 국소적인 상호작용들만 존재하며, 그 함수가 이동에 대해 equivariant 함을 나타낸다. 마찬가지로 풀링을 사용한다는 것은 각 단위가 작은 이동에 대해 invariant 해야 함을 뜻하는 무한히 강한 사전분포를 도입하는 것을 의미한다.

### 통찰

합성곱 신경망을 무한히 강한 사전분포가 있는 완전 연결 신경망으로 구현할 수도 있겠으나 계산 비용 측면에서 큰 비용이 발생한다. 그러나 완전 연결 신경망의 관점에서 합성곱 신경망의 작동 방식에 대해 고찰하면 통찰을 얻을 수 있다.

핵심적인 통찰 하나는, 합성곱과 풀링이 **과소적합**을 일으킬 수 있다는 것이다. 다른 모든 사전분포와 마찬가지로, 합성곱과 풀링은 그 사전분포의 가정이 실제로 어느정도 성립할 때만 유용하다. 예를들어 **정확한 공간 정보를 유지하는 것이 중요한 과제를 푸는 경우, 모든 특징에 풀링을 적용하면 훈련 오차가 증가할 수 있다.** 합성곱 신경망 아키텍처 중에는 특징들의 불변성을 높임과 동시에, 이동 불변성 사전분포의 가정이 성립하지 않아도 특징들에 대해 과소적합이 일어나지 않도록 일부 채널에만 풀링을 사용하고 그 외에 채널에는 풀링을 사용하지 않는 것들도 있다.

다른예로, **입력 공간 안에서 아주 멀리 떨어진 곳에 있는 정보를 가져와야 하는 과제에는 합성곱이 뜻하는 사전분포가 적절하지 않을 수 있다.**

### Reference

[https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)