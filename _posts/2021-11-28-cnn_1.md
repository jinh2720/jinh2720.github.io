---
layout: single
title: "CNN 특성을 살펴보자"
author_profile: true
excerpt: "sparse interaction, parameter share, equivariant representation"
categories:
    - CNN
---

### CNN은 무엇인가

CNN의 의미를 생각해보기 위해서 단어를 뜯어보도록 하자. 

"Convolutional Neural Network" 먼저 뒤에 붙은 **"Neural Network"**는 신경망을 의미한다. 이름 때문에 실제 우리 뉴런의 작동방식과 같다라고 오해를 한다고 한다. 하지만 뉴런들 간에 전기신호를 전달하는 형태와 유사하기 때문이지 더 자세한 작동방식에 있어서 동일하다는 의미는 아니라고 한다. 어쨌거나 여기서 Neural Network는 실제 인간의 뉴런의 구조적인 모습과 유사하여 이름이 붙여졌다고 생각하면 된다.

다음은 **"Convolution"** 의 의미이다. **"합성곱"** 이라고 하는데, 위키백과를 보면 **합성곱**, 또는 **콘벌루션**은 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 수학 연산자이다. 

분명히 어려운 용어는 없는데 잘 이해가 안된다. 그래서 연산과정을 생각해보면 굳이 수학적으로 이해하지 못하더라도 의미를 이해할 수 있다. 쉽게 말해서 **두 행렬의 곱**이라고 말할 수 있다.

여기서 Input 이미지가 하나의 행렬이 되고, kernel 또는 filter 라고 하는 것이 하나의 행렬이다. 결국 CNN은 두 행렬, input과 kernel 행렬을 반복해서 연산하는 네트워크라고 설명할 수 있다.

전통적인 머신러닝에서는 feature engineering 이라고 해서 데이터의 유의미한 Feature map 생성을 엔지니어의 수작업으로 이루어졌다. 하지만 CNN은 이렇게 중요한 feature engineering 작업을 자동화시켜버린다. 즉, feature map을 생성해 주는 filter를 학습하는 모델이 CNN인 것이다.

### CNN의 가정

"Stationarity of statstics and locality of pixel dependencies"

stationarity는 통계적인 특성이 시간이 지나도 변하지 않는다는 뜻이고(여기서는 위치에 관계없이 패턴으 변하지 않음으로 이해), locality는 이미지의 어느 한 특성은 그 주변과 관련성이 있다는 의미이다.

### CNN의 특성

**sparse interaction (sparse weights, sparse connectivity)**

sparse interation은 이전 layer의 output 다음 layer 의 input이 될 때, 완전히 layer 간에 연결(그림:위쪽)이 되는 것이 아니라 근처 노드의 일부(그림:아래쪽)만 연결이 된다. 이렇게 하면 완전히 연결된 경우에 비해서 훨씬 더 적은 특성을 학습하게 된다. 그럼에도 불구하고 CNN에서 이 방식을 이용하는 이유는 모델의 복잡도가 줄어들기 때문이다. 복잡도가 줄어들 뿐만 아니라 이미지의 특성을 반영한 이유도 있다. 그것은 우리가 고양이를 분류하는 모델을 학습한다고 생각해보면, 고양이의 눈, 코, 입, 꼬리 등을 찾을 것이다. 이때 눈을 찾을 때는 꼬리에서 있는 부분을 고려할 필요가 없고 그 반대도 마찬가지이다. 즉, 유의미한 feature 찾을 때, Local 영역에 있는 데이터만 활용한다는 의도이다. 

![sparse interaction.PNG](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9dd56c9b-127f-4f7a-ba8e-017aaa250823/sparse_interaction.png)

**parameter share (**왜 parameter share라는 아이디어를 생각하게 됐을까?**)**

parameter sharing은 결국 네트워크의 효율성에 기인한다. 위치에 따라 feature를 구분하는 것보다 위치에 상관없이 feature를 추출하는 것이 훨씬 더 효과적이다. 예를들어 고양이가 어느 위치에 있던 간에 고양이 feature를 찾아내는 것처럼 말이다.

![prameter share.PNG](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b68bbdde-3fbe-45fe-a966-91e3dc9ecf5e/prameter_share.png)

**equivariant representation**

quivariant라는 의미는 함수의 입력값이 바뀌면 결과값 또한 바뀌는 것을 의미한다. parameter share 의 특성을 잘 생각해보면, 이미지가 traslation이 되었을 때, 동일한 가중치가 여러 위치에서 활용이 되기 때문에 이동된 양만큼 output에 반영된다.

### Translation equivariance vs Translation invariance

위에서 CNN의 equivariant한 특성을 살펴보았다. 조금 헷갈릴 수 있는데, 우리가 흔히 알고 있는 CNN은 translation invariance 특성을 가진다. 무슨소리냐? CNN 내부에서 Convolution하는 과정에서는 equivariance 특성에 따라 위치 정보가 반영이 된다. 하지만 FCL 이후 Classifier를 통해 나온 분류 결과는 invariance 하다는 의미이다. 자세한 설명은 아래 블로그에 잘 설명 되어 있으니 참고바란다.

[https://seoilgun.medium.com/cnn의-stationarity와-locality-610166700979](https://seoilgun.medium.com/cnn%EC%9D%98-stationarity%EC%99%80-locality-610166700979)