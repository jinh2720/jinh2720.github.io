---
layout: single
title: "TFRecord를 이용한 학습 (2)"
author_profile: true
excerpt: "data pipeline"
categories:
    - MLOps
---

지난번에 살펴본 "TFRecod 를 이용한 학습 (1)"에서 작성한 코드 중 **데이터 파이프라인** 생성과정에 대해서 자세히 알아보자. 

```python
dataset = dataset.map(_parse_image_function)
    dataset = dataset.shuffle(buffer_size=100000, seed=None, reshuffle_each_iteration=None)
    dataset = dataset.repeat(count=None)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(2)
```



## map()

```python
dataset = dataset.map(_parse_image_function)
```

map 함수는 차례대로 입력 데이터 셋으로부터 하나의 요소를 나타내는 텐서 객체를 취해서 새롭게 변형된 텐서 객체를 반환한다. 출력의 요소의 순서는 입력 순서와 동일하다.

아래 소스코드를 참고바란다.

```python
ds = tf.data.Dataset.range(1,6) # [1,2,3,4,5]
ds.map(lambda x: x+1) # [2,3,4,5,6]
```

 

## shuffle()

```python
dataset = dataset.shuffle(buffer_size=100000, seed=None, reshuffle_each_iteration=None)
```

shuffle 함수는 데이터 셋을 랜덤하게 섞는다. 데이터 셋을 특정한 buffer_size의 요소들로 버퍼를 채우고, 선택한 요소들은 변경해 가며 이 버퍼로부터 랜덤하게 요소들을 샘플링 한다. 완벽한 셔플링을 위해서는 **버퍼의 크기가 데이터 셋 전체의 크기보다 크거나 같아야 한다.**



## repeat()

```python
dataset = dataset.repeat(count=None)
```

repeat 함수는 데이터 셋이 섞이고 나서, 데이터를 모델에 반복적으로 제공해주어야 한다. 이것을 위해 사용하는 것이고, 모델이 파이프라인 내에서 항상 셔플링 된 데이터의 배치를 확볼 할 수 있도록 해준다.



## batch()

```python
dataset = dataset.batch(batch_size, drop_remainder=True)
```

경사 하강법은 머신러닝과 딥러닝에서 널리 쓰이는 학습 알고리즘이다. 경사하강법에는 3가지 알고리즘이 있다.

- 모든 데이터를  이용하여 학습하는 **배치 경사 하강법**(Batch gradient descent) 

- 데이터의 배치를 이용하여 학습하는 **미니-배치 경사 하강법**(Mini-batch gradient descent)
- 랜덤하게 셔플링된 데이터를 이용하여 학습하는 **확률적 경사 하강법** (Stochastic gradient descent) 

하드웨어적으로 비효율적인 배치 경사 하강법이나 시간이 오래 걸리는 확률적 경사 하강법에 비해 미니-배치 경사 하강법이 가장 널리 이용 된다. 배치크기는 GPU(CPU) 의 메모리를 벗어나지 않도록 설정해주는 것이 중요하다.



## prefetch()

```python
dataset = dataset.prefetch(2)
```

현재 배치가 모델 학습에 이용되는 동안 데이터의 다음 배치를 프리패치한다. batch_size 보다 커야 한다. prefetch()가 batch() 이후에 선언이 된다면 1은 하나의 배치를 의미한다.